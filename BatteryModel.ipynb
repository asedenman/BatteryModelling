{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batteries\n",
    "\n",
    "### Concept A: a battery which trades power with the grid.\n",
    "Considering it is possible to sell power back to the grid in the Uk, it may be possible to play the market - to charge a battery, or battery network, during quiet hours and discharge the battery during peak hours - essentially buying low and selling high.<br><br>\n",
    "\n",
    "Issues:\n",
    "1. Contractual supply - Power suppliers (coal power plants etc.) are, as I understand, contractually obliged to provide a certain amount of power to the grid. Failing to meet this requirement means the suppliers pay the grid for the shortfall. I do not know what the contractual requirements/regulations would be for a battery storage facility, but, in order for this concept to work, they would have to be flexible enough to allow for relatively free buying and selling power. I believe a large number of such storage facilities would have a dampening effect on short-term market fluctuations, and may have a stabilising effect on the cost of power - if so, then the national grid may take a favourable view.\n",
    "2. Free market equilibrium - if we had a large number of battery storage facilities, then, the market would shift in such a way that the predictable low and highs in price (during the night etc.) would no longer be as profitable. I.e., the increased demand during the night would push the price up, and the increased supply during the day would push the price  down. The idealised equilibrium would likely be dependant on battery efficiency and charge decay.\n",
    "3. Moral concerns - we need battery storage for a worldwide shift to renewable power to feasible. Introducing battery power trading might be against the spirit of green energy, even though it may have positive effects\n",
    "\n",
    "### Concept B: Combined power management\n",
    "A system designed to link a battery, on-site power generation, and mains power. <br>For example, a house with solar panels and a large battery. The system, using a predictive model based partly on meteorological data, could determine that the estimated power generation from the solar panels in the upcoming day will not be sufficient to cover the estimated use. The system could then decide to use mains power to charge the batteries at night, when power is cheaper. This could allow for some manual control/prompting - e.g. if the resident expects to take a day off/go on holiday - then the estimated power use would change. This could also link with a smart meter and account for a electric car.\n",
    "* This would require a model predicting the consumer cost of power, a model for on-site power generation, and a model for predicting daily power use.\n",
    "\n",
    "### Prediction model\n",
    "A few notes regarding a potential predictive model for the price of power:\n",
    "1. It seems the best data source would be Elexon - in the UK at least. They also provide data regarding estimated solar and wind power generation, and have a public access api\n",
    "2. There are a few financial models out there which would likely serve as a good basis such as <a href=\"url\" target=\"https://www.researchgate.net/publication/324802031_Algorithmic_Financial_Trading_with_Deep_Convolutional_Neural_Networks_Time_Series_to_Image_Conversion_Approach\">this</a>\n",
    "\n",
    "### The Battery Model\n",
    "This model is intended to determine the best times to buy, sell, or hold battery power.<br>It is not a predictive model of price, as that is a little outside the scope of this notebook.\n",
    "<br>Although I started working on this with Concept A in mind, it could be adjusted to work with B - rewarding maximising savings<br>\n",
    "\n",
    "* The model is a deep q-learning neural network, using two networks - a target network and training network. It also uses a random sampling for back propogation.\n",
    "* The data was taken from the Elexon Portal\n",
    "\n",
    "*Unfortunately, my PC is running out of memory trying to train the model and I've not had time to try to resolve it*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import sys\n",
    "\n",
    "DATA_PATH = './data/sspsbpniv.csv'\n",
    "LEARNING_RATE = 0.01\n",
    "REPLAY_BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    df['Settlement Date'] = pd.to_datetime(df['Settlement Date'], dayfirst=True)\n",
    "    df.rename(columns={'System Sell Price(£/MWh)':'Sell Price'}, inplace=True)\n",
    "\n",
    "    df['Period (sin norm)'] = np.sin((df['Settlement Period'] - 1 )* (2 * np.pi / 48))\n",
    "    df['Period (cos norm)'] = np.cos((df['Settlement Period'] - 1 )* (2 * np.pi / 48))\n",
    "\n",
    "    # shifted Ratio of Sell Price to Rolling average \n",
    "    df['SP/SMA'] = (df['Sell Price'] / df['Sell Price'].rolling(48).mean() ) - 1\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop(['Settlement Date', 'System Buy Price(£/MWh)', 'Net Imbalance Volume(MWh)', 'Settlement Period'], inplace=True, axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, is_eval=False, model_name=''):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = 3 # sit, buy, sell\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.is_eval = is_eval\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = load_model(model_name) if model_name != '' else self._model()\n",
    "        self.target_model = self._model() if not is_eval else None\n",
    "\n",
    "    def _model(self):\n",
    "        # not optimised\n",
    "        model = Sequential()\n",
    "        # input layer\n",
    "        model.add(Dense(units=64, input_dim=self.state_size, activation=\"relu\"))\n",
    "        # hidden layers\n",
    "        model.add(Dense(units=32, activation=\"relu\"))\n",
    "        model.add(Dense(units=8, activation=\"relu\"))\n",
    "        # output layer\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        if not self.is_eval and random.random()<= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        return np.argmax(self.model.predict(state.reshape(1, 5), verbose=0)[0])\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "    \n",
    "    def replay(self):\n",
    "        batch_size = REPLAY_BATCH_SIZE\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        # random sampling of memory\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        # TODO review loop - model.predict is intended to operate on batches\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            # what does the target model predict\n",
    "            target = self.target_model.predict(state.reshape(1, 5), verbose=0)\n",
    "            # if no new state in sample, then Q(S_t+1) = 0\n",
    "            if(done):\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                q_new = max(self.target_model.predict(new_state.reshape(1, 5), verbose=0)[0])\n",
    "                target[0][action] = reward + q_new *self.gamma\n",
    "            self.model.fit(state.reshape(1, 5), target, epochs=1, verbose=0)\n",
    "            \n",
    "    def update_target(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i]\n",
    "        self.target_model.set_weights(target_weights)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatPrice(n):\n",
    "    return(\"-£\" if n<0 else \"£\")+\"{0:.2f}\".format(abs(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATTERY_MAX_CHARGE = 1000\n",
    "BATTERY_CHARGE_RATE = 100\n",
    "BATTERY_CHARGE_EFF = 0.9\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, states, prices) -> None:\n",
    "        self.states = states\n",
    "        self.prices = prices\n",
    "        self.current_charge = 0\n",
    "        self.max_charge = BATTERY_MAX_CHARGE\n",
    "        self.charge_eff = BATTERY_CHARGE_EFF\n",
    "        self.sc = self.charge_eff # sell cost/discharge efficiency\n",
    "        self.bc = 1 / self.charge_eff #buy cost/charge efficiency\n",
    "        self.sum_cost = 0\n",
    "        self.profit = 0\n",
    "        self.rate = BATTERY_CHARGE_RATE\n",
    "        self.cur_index = 0\n",
    "        self.length = states.shape[0] - 1\n",
    "            \n",
    "    def mean_value(self)->float:\n",
    "        try:\n",
    "            return self.sum_cost / self.current_charge\n",
    "        except ZeroDivisionError:\n",
    "            return 0\n",
    "    \n",
    "    def cur_price(self) -> float:\n",
    "        return self.prices[self.cur_index]\n",
    "    \n",
    "    def comp_mean_to_current(self)->float:\n",
    "        try: \n",
    "            return self.cur_price()/self.mean_value()\n",
    "        except ZeroDivisionError:\n",
    "            return 1\n",
    "    \n",
    "    def get_state(self):\n",
    "        state = self.states.iloc[self.cur_index].to_numpy()\n",
    "        return np.append(state, \n",
    "                         [self.current_charge / self.max_charge, \n",
    "                          self.comp_mean_to_current()])\n",
    "    \n",
    "    def get_new_state(self):\n",
    "        self.cur_index += 1\n",
    "        return self.get_state()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cur_index = 47\n",
    "        self.current_charge = random.randint(0, self.max_charge)\n",
    "        self.sum_cost = self.current_charge * self.cur_price()\n",
    "        self.profit = -self.sum_cost\n",
    "    \n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = self.simple_reward(action, self.cur_price())\n",
    "        return  (self.get_new_state(),\n",
    "                 reward,\n",
    "                 self.cur_index >= self.length)\n",
    "    \n",
    "    \n",
    "    def simple_reward(self, action, price) -> float:\n",
    "        reward = 0.\n",
    "        if action == 0: # hold\n",
    "            pass\n",
    "        elif action == 1 and self.current_charge < self.max_charge: # buy\n",
    "            charge = self.rate if self.current_charge + self.rate <= self.max_charge \\\n",
    "                else self.max_charge - self.current_charge\n",
    "            cost = price * charge * self.bc\n",
    "            self.sum_cost += cost\n",
    "            self.profit -= cost\n",
    "            self.current_charge += charge\n",
    "            # print(\"Buy: \" + formatPrice(price))\n",
    "        elif action == 2 and self.current_charge > 0: # sell\n",
    "            charge = self.rate if self.current_charge - self.rate >= 0 \\\n",
    "                else self.current_charge\n",
    "            # we use the average value of held charge for reward\n",
    "            average = self.mean_value()\n",
    "            self.sum_cost -= average * charge\n",
    "            self.current_charge -= charge\n",
    "            sum_sell_price = price * charge * self.sc\n",
    "            self.profit += sum_sell_price\n",
    "            reward = max((sum_sell_price) - average, 0)\n",
    "            # print(\"Sell: \" + formatPrice(price) + \" | Profit: \" + formatPrice(reward))\n",
    "        return reward\n",
    "            \n",
    "    def print_profit(self):\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"Total Profit: \" + formatPrice(self.profit))\n",
    "        print(\"--------------------------------\")\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(states, prices):\n",
    "    env = Environment(states, prices)\n",
    "    agent = Agent(5, False)\n",
    "    \n",
    "    models_path = []\n",
    "    \n",
    "    episode_count = 10\n",
    "    l = len(states) - 1\n",
    "    \n",
    "    # Adding a hard limit for testing\n",
    "    max_steps = 100\n",
    "    tenth = max_steps / 10\n",
    "    \n",
    "    for e in range(episode_count + 1):\n",
    "        print(\"Episode \" + str(e) + \"/\" + str(episode_count))\n",
    "        env.reset()\n",
    "        state = env.get_state()\n",
    "        total_profit = 0\n",
    "        for t in range(l):\n",
    "            action = agent.act(state)\n",
    "            new_state, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, new_state, done)\n",
    "            agent.replay()\n",
    "            agent.update_target()\n",
    "            state = new_state\n",
    "            \n",
    "            if t > max_steps:\n",
    "                done = True\n",
    "            \n",
    "            if t > 0 and t % tenth == 0:\n",
    "              print(f'{t/max_steps:0%} complete')\n",
    "            \n",
    "            if done:\n",
    "                env.print_profit()\n",
    "                break\n",
    "    if e % 10 == 0:\n",
    "        agent.model.save(str(e))\n",
    "        models_path.append(str(e))\n",
    "    \n",
    "    return models_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(states, prices, model_path):\n",
    "    env = Environment(states, prices)\n",
    "    agent = Agent(5, True, model_path)\n",
    "    l = len(states) - 1\n",
    "    print(\"Model \" + str(model_path))\n",
    "    env.reset()\n",
    "    state = env.get_state()\n",
    "    total_profit = 0\n",
    "    for t in range(l):\n",
    "        action = agent.act(state)\n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        state = new_state\n",
    "        if done:\n",
    "            env.print_profit()\n",
    "    \n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Model \" + str(model_path))\n",
    "    print(\"Total Profit: \" + formatPrice(env.profit))\n",
    "    print(\"--------------------------------\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "# main\n",
    "df = load_data()\n",
    "prices = df.pop('Sell Price')\n",
    "states = df\n",
    "models = train(states, prices)\n",
    "\n",
    "for name in models:\n",
    "    eval(states, prices, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3858c12c8a3d1fb2e7e139d08746de36d9c2da6dc52351386ba026ce912f4249"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
